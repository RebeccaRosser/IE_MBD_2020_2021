{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE MBD APR 2020: NLP Group Project (Group D)\n",
    "\n",
    "## Topic Modelling the Quora Question Bank using LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "### Group D:\n",
    "\n",
    "+ Alain Grullón\n",
    "+ Alexandre Bouamama\n",
    "+ Guillermo Germade\n",
    "+ Rebecca Rosser\n",
    "+ Roberto Picón\n",
    "+ Tarek El Noury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data from the Quora Questions dataset\n",
    "data = pd.read_csv('This is the quora dataset.csv', encoding= 'unicode_escape')\n",
    "data.columns = ['id', 'question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['question']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "question    44\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "question    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195980</th>\n",
       "      <td>What is the most effective way to learn Spanish?</td>\n",
       "      <td>195980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60848</th>\n",
       "      <td>How do I recover deleted photos from an Androi...</td>\n",
       "      <td>60848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282570</th>\n",
       "      <td>How can I see who viewed my instagram?</td>\n",
       "      <td>282570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155866</th>\n",
       "      <td>How electric field is produced by magnetic field?</td>\n",
       "      <td>155866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127223</th>\n",
       "      <td>Is it possible to prepare for IES during an MT...</td>\n",
       "      <td>127223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question   index\n",
       "195980   What is the most effective way to learn Spanish?  195980\n",
       "60848   How do I recover deleted photos from an Androi...   60848\n",
       "282570             How can I see who viewed my instagram?  282570\n",
       "155866  How electric field is produced by magnetic field?  155866\n",
       "127223  Is it possible to prepare for IES during an MT...  127223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will work with the documents DataFrame. Here's a quick look at 5 random rows:\n",
    "documents.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    44\n",
       "index        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Number of missing values\n",
    "\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404286 entries, 0 to 404285\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   question  404242 non-null  object\n",
      " 1   index     404286 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# And here's a look at the amount and types of data we have at our disposal:\n",
    "documents.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "#### We will perform the following steps for Question Selection:\n",
    "\n",
    "\n",
    "\n",
    "+ **Tokenization**: Split the questions into words, splitting by whitespace ' '.\n",
    "\n",
    "+ **Question Selection**: We will observe the distribution of tokens in the questions dataset and crop off questions with relatively low amount of tokens, as they have less information for the LDA to be accurate and are also less likely to be representative of people seeking help, which is our ultimate goal for questions in our app.\n",
    "\n",
    "+ **Null values**: We will take care of them by simply dropping them, as we do not need them since we have enough data for our purpose of finding topic clusters to categorize the Quora Questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    44\n",
       "index        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Number of missing values\n",
    "\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing by splitting questions using whitespace: ' ' \n",
    "tokens = []\n",
    "for doc in documents[\"question\"].apply(str):\n",
    "    tokens.append(doc.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the tokens column to the DataFrame\n",
    "documents[\"tokens\"] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional column to measure the count of tokens per question (length of lists, or count of items in lists)\n",
    "documents[\"tokens_cnt\"] = documents.tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      44\n",
       "index          0\n",
       "tokens         0\n",
       "tokens_cnt     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding number of null values\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null values\n",
    "documents = documents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      0\n",
       "index         0\n",
       "tokens        0\n",
       "tokens_cnt    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying absence of nulls\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135977</th>\n",
       "      <td>Can u provide me with daily news?</td>\n",
       "      <td>135977</td>\n",
       "      <td>[Can, u, provide, me, with, daily, news?]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202274</th>\n",
       "      <td>How do you earn money on Quora?</td>\n",
       "      <td>202274</td>\n",
       "      <td>[How, do, you, earn, money, on, Quora?]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250265</th>\n",
       "      <td>Is there any limit to earn money from YouTube ...</td>\n",
       "      <td>250265</td>\n",
       "      <td>[Is, there, any, limit, to, earn, money, from,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86391</th>\n",
       "      <td>How can I know whether or not I am a genius?</td>\n",
       "      <td>86391</td>\n",
       "      <td>[How, can, I, know, whether, or, not, I, am, a...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279147</th>\n",
       "      <td>How old are y'all? I just turned 45.</td>\n",
       "      <td>279147</td>\n",
       "      <td>[How, old, are, y'all?, I, just, turned, 45.]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question   index  \\\n",
       "135977                  Can u provide me with daily news?  135977   \n",
       "202274                    How do you earn money on Quora?  202274   \n",
       "250265  Is there any limit to earn money from YouTube ...  250265   \n",
       "86391        How can I know whether or not I am a genius?   86391   \n",
       "279147               How old are y'all? I just turned 45.  279147   \n",
       "\n",
       "                                                   tokens  tokens_cnt  \n",
       "135977          [Can, u, provide, me, with, daily, news?]           7  \n",
       "202274            [How, do, you, earn, money, on, Quora?]           7  \n",
       "250265  [Is, there, any, limit, to, earn, money, from,...          12  \n",
       "86391   [How, can, I, know, whether, or, not, I, am, a...          11  \n",
       "279147      [How, old, are, y'all?, I, just, turned, 45.]           8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the current look of the documents DF, with tokens and tokens_cnt added.\n",
    "documents.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F5C0A62130>]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We believe that questions with low token amounts are usually conveying little information, \n",
    "# and perhaps are hurtful distinguishing between good clusters of topics. So let's look at token count distribution:\n",
    "\n",
    "binsize = 20\n",
    "display(documents.hist('tokens_cnt', bins = binsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.921, 4.9]     19361\n",
       "(4.9, 8.8]      151921\n",
       "(8.8, 12.7]     142470\n",
       "(12.7, 16.6]     55178\n",
       "(16.6, 20.5]     20270\n",
       "(20.5, 24.4]      8377\n",
       "(24.4, 28.3]      4281\n",
       "(28.3, 32.2]      1519\n",
       "(32.2, 36.1]       427\n",
       "(36.1, 40.0]       203\n",
       "(40.0, 43.9]        96\n",
       "(43.9, 47.8]        51\n",
       "(47.8, 51.7]        41\n",
       "(51.7, 55.6]        21\n",
       "(55.6, 59.5]        12\n",
       "(59.5, 63.4]         6\n",
       "(63.4, 67.3]         6\n",
       "(67.3, 71.2]         0\n",
       "(71.2, 75.1]         1\n",
       "(75.1, 79.0]         1\n",
       "Name: tokens_cnt, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now looking at individual bin sizes:\n",
    "display(documents.tokens_cnt.value_counts(bins=binsize).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this distribution, we believe it would make sense to crop off the left tail of questions (the ones with <= 6 tokens) since they are the least helpful for our task.\n",
    "\n",
    "Let's see how much of a percentage these tokens represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.921, 4.9]     19361\n",
       "(4.9, 8.8]      151921\n",
       "(8.8, 12.7]     142470\n",
       "(12.7, 16.6]     55178\n",
       "(16.6, 20.5]     20270\n",
       "(20.5, 24.4]      8377\n",
       "(24.4, 28.3]      4281\n",
       "(28.3, 32.2]      1519\n",
       "(32.2, 36.1]       427\n",
       "(36.1, 40.0]       203\n",
       "(40.0, 43.9]        96\n",
       "(43.9, 47.8]        51\n",
       "(47.8, 51.7]        41\n",
       "(51.7, 55.6]        21\n",
       "(55.6, 59.5]        12\n",
       "(59.5, 63.4]         6\n",
       "(63.4, 67.3]         6\n",
       "(67.3, 71.2]         0\n",
       "(71.2, 75.1]         1\n",
       "(75.1, 79.0]         1\n",
       "Name: tokens_cnt, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(documents.tokens_cnt.count())\n",
    "documents.tokens_cnt.value_counts(bins=20).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1861657126176894"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.tokens[documents.tokens_cnt <= 6].count()/documents.tokens.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~18.6% of the dataset, we think we can drop them while retaining a satisfactory amount of data for this proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping the questions with <=6 tokens\n",
    "documents = documents[~(documents.tokens_cnt <= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the DataFrame index as well as the index column\n",
    "documents = documents.reset_index()\n",
    "documents[\"index\"] = documents.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_0       328986\n",
       "question      328986\n",
       "index         328986\n",
       "tokens        328986\n",
       "tokens_cnt    328986\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the remaining rows\n",
    "documents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing Functions\n",
    "\n",
    "#### Now we will make functions that perform the following steps\n",
    "\n",
    "+ **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation. \n",
    "\n",
    "+ **All stopwords are removed**. Words that have 3 or fewer characters are removed as well, even if not in the gensim list of stopwords.\n",
    "\n",
    "+ **Lemmatization**: words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "+ **Stemmization**: words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Guillermo\n",
      "[nltk_data]     Germade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to perform lemmatize and stem preprocessing steps on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):    \n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "What is the best profession for me? I love to travel the world and I am 25 now.\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['best', 'profess', 'love', 'travel', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Checking that the preprocessing is working correctly on a random sample.\n",
    "# Feel free to play with the sample_index value\n",
    "\n",
    "sample_index = 200000\n",
    "doc_sample = documents.question[sample_index]\n",
    "\n",
    "print('original document: ')    \n",
    "print(doc_sample)\n",
    "\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It worked!\n",
    "\n",
    "#### Preprocess the questions, saving the results as ‘processed_docs’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [step, step, guid, invest, share, market, india]\n",
       "1                    [stori, kohinoor, noor, diamond]\n",
       "2                 [increas, speed, internet, connect]\n",
       "3                                [mental, lone, solv]\n",
       "4                       [dissolv, water, quik, sugar]\n",
       "5                   [astrolog, capricorn, moon, rise]\n",
       "6                                   [good, geologist]\n",
       "7                                           [instead]\n",
       "8       [motorola, compani, hack, charter, motorolla]\n",
       "9             [method, separ, slit, fresnel, biprism]\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['question'].apply(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set\n",
    "\n",
    "#### Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(37569 unique tokens: ['guid', 'india', 'invest', 'market', 'share']...)\n"
     ]
    }
   ],
   "source": [
    "# Looking at the amount of unique tokens\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 0,\n",
       " 'india': 1,\n",
       " 'invest': 2,\n",
       " 'market': 3,\n",
       " 'share': 4,\n",
       " 'step': 5,\n",
       " 'diamond': 6,\n",
       " 'kohinoor': 7,\n",
       " 'noor': 8,\n",
       " 'stori': 9}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at some of the token unique integer IDs. Feel free to play around with number_samples to see more\n",
    "\n",
    "number_samples = 10\n",
    "dict(list(dictionary.token2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim filter_extremes\n",
    "\n",
    "#### Filter out tokens that appear in\n",
    "+ less than 10 documents (insufficient/unreliable datapoints) or\n",
    "+ more than 50% of documents (probably bad for categorizing, as they're found across many topics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "+ For each document we create a dictionary reporting how many words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'list'>     404198\n",
       "<class 'float'>        44\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L =[]\n",
    "for i in documents.tokens:\n",
    "    L.append(type(i))\n",
    "\n",
    "pd.Series(L).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# documents[\"token_count\"] = documents.tokens.apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(834, 1), (1299, 1), (1668, 1), (2633, 1), (3376, 1), (4073, 1), (4880, 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 834 (\"control\") appears 1 time.\n",
      "Word 1299 (\"case\") appears 1 time.\n",
      "Word 1668 (\"loss\") appears 1 time.\n",
      "Word 2633 (\"airbus\") appears 1 time.\n",
      "Word 3376 (\"remain\") appears 1 time.\n",
      "Word 4073 (\"detach\") appears 1 time.\n",
      "Word 4880 (\"vertic\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_sample = bow_corpus[100000]\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], \n",
    "                                              dictionary[bow_doc_sample[i][0]],\n",
    "                                              bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "+ Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.42689670967641136),\n",
      " (1, 0.18518456364563315),\n",
      " (2, 0.3019869755675633),\n",
      " (3, 0.2819330679993177),\n",
      " (4, 0.33782668106587427),\n",
      " (5, 0.7061562305686286)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "+ Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.017*\"think\" + 0.015*\"note\" + 0.014*\"trump\" + 0.014*\"peopl\" + 0.013*\"indian\" + 0.012*\"lose\" + 0.012*\"happen\" + 0.010*\"india\" + 0.010*\"presid\" + 0.009*\"weight\"\n",
      "Topic: 1 \n",
      "Words: 0.073*\"best\" + 0.034*\"india\" + 0.025*\"learn\" + 0.016*\"money\" + 0.015*\"onlin\" + 0.013*\"program\" + 0.012*\"phone\" + 0.012*\"account\" + 0.012*\"movi\" + 0.011*\"number\"\n",
      "Topic: 2 \n",
      "Words: 0.026*\"best\" + 0.015*\"time\" + 0.014*\"english\" + 0.012*\"good\" + 0.012*\"place\" + 0.009*\"word\" + 0.009*\"instagram\" + 0.009*\"iphon\" + 0.009*\"card\" + 0.008*\"differ\"\n",
      "Topic: 3 \n",
      "Words: 0.030*\"differ\" + 0.026*\"quora\" + 0.021*\"thing\" + 0.021*\"know\" + 0.021*\"life\" + 0.020*\"question\" + 0.019*\"peopl\" + 0.014*\"girl\" + 0.012*\"love\" + 0.012*\"need\"\n",
      "Topic: 4 \n",
      "Words: 0.037*\"like\" + 0.017*\"engin\" + 0.016*\"book\" + 0.014*\"world\" + 0.012*\"univers\" + 0.012*\"best\" + 0.011*\"prepar\" + 0.010*\"year\" + 0.010*\"good\" + 0.009*\"student\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.017*\"money\" + 0.011*\"trump\" + 0.011*\"best\" + 0.010*\"note\" + 0.010*\"lose\" + 0.009*\"think\" + 0.009*\"india\" + 0.009*\"weight\" + 0.008*\"donald\" + 0.008*\"presid\"\n",
      "Topic: 1 Word: 0.023*\"quora\" + 0.016*\"question\" + 0.014*\"movi\" + 0.014*\"best\" + 0.010*\"peopl\" + 0.010*\"answer\" + 0.008*\"facebook\" + 0.008*\"account\" + 0.008*\"instagram\" + 0.007*\"watch\"\n",
      "Topic: 2 Word: 0.013*\"engin\" + 0.010*\"best\" + 0.007*\"studi\" + 0.006*\"android\" + 0.006*\"card\" + 0.006*\"student\" + 0.005*\"colleg\" + 0.005*\"good\" + 0.005*\"youtub\" + 0.005*\"scienc\"\n",
      "Topic: 3 Word: 0.017*\"know\" + 0.016*\"thing\" + 0.015*\"life\" + 0.013*\"like\" + 0.012*\"love\" + 0.010*\"girl\" + 0.009*\"best\" + 0.009*\"mean\" + 0.007*\"market\" + 0.007*\"feel\"\n",
      "Topic: 4 Word: 0.014*\"learn\" + 0.013*\"best\" + 0.013*\"differ\" + 0.011*\"improv\" + 0.010*\"program\" + 0.010*\"languag\" + 0.009*\"english\" + 0.008*\"websit\" + 0.008*\"busi\" + 0.007*\"start\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 6145 #Change to whatever you want\n",
    "print(\"The original question is: <<\", documents.iloc[val,0], \">>\")\n",
    "print()\n",
    "print(\"The preprocessed document is:\", processed_docs[val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[6126]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
